{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH16_PART0_with_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaVm/J80xmzM9MGqmQmPD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qamtam/Hands-on-machine-learning/blob/main/CH16_PART0_with_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_txpnjRtd8Y"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"cnn\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXU3X5nttZN"
      },
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzrqdZ_tuAHn"
      },
      "source": [
        "#encode every character  as an integer\n",
        "#tokenizer koduje każdy kolejny znak jako inne ID od 1 do liczby znaków\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([shakespeare_text])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKWc5Y3Bub11",
        "outputId": "83d60614-5269-405f-8638-06057081659f"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"yo mmamma\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[16, 4, 1, 15, 15, 5, 15, 15, 5]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MY0vlubuiYW"
      },
      "source": [
        "max_id = len(tokenizer.word_index)\n",
        "dataset_size = tokenizer.document_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xdn-BYDup2t"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1 #odejmujemy 1, żeby zaczynać od \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eSjrrWEuyxg",
        "outputId": "22a5fa0a-48c0-46e7-e536-85f983528555"
      },
      "source": [
        "encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19,  5,  8, ..., 20, 26, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6kHqhKuzgM"
      },
      "source": [
        "# dzielenie powinno być zwykle w wymiarze czasu, szczególnie gdy mamy stacjonarność\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QkengX1xALl"
      },
      "source": [
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-3B87p1xmWU"
      },
      "source": [
        "#dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "# 1) każdy z \"wewnętrznych datasetów\" najpierw batchujemy, żeby ??uzyskać jeden tensor z każdego okna?? ze względu na to, że każde okno ma tę samą długość dostaniemy jeden tensor na jedno okno\n",
        "# 2) pozbywamy się \"wewnętrznych datasetów\", żeby zostały gołe tensory\n",
        "#batch_size = 32\n",
        "#dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "#dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) #stworzenie targetu\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rMhG09TyetU"
      },
      "source": [
        "#zakodowanie datasetu do one-hot\n",
        "#map(lambda nowe: operacja(stare))\n",
        "#dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch))\n",
        "#coś zjebałem przy przepisywaniu\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlNaByFf1WDh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEIiZ_cf1nbQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf67oKVn2J96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPpuknL2ra0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iMKAzj12sno",
        "outputId": "51d67f24-1256-479b-ab98-3d1e5c7f586e"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "    \n",
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)\n",
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 23.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 17.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 15.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 12.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 11.5MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 12.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 12.0MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 11.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 11.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 11.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0MB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0MB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0MB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 11.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI4oRj9Z3zt2",
        "outputId": "c559c28a-5356-4d80-ace3-46bfc6f0b794"
      },
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch, Y_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]]], shape=(32, 100, 39), dtype=float32) tf.Tensor(\n",
            "[[ 5  7  0 ...  0 18  4]\n",
            " [ 1  0 24 ...  0 11  1]\n",
            " [12  0 21 ... 29 10 10]\n",
            " ...\n",
            " [27  0  2 ...  7  3 19]\n",
            " [ 3  9 12 ...  4  9  7]\n",
            " [ 7  0  3 ...  8  1  0]], shape=(32, 100), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udhVvsdt37ZW",
        "outputId": "22a9cf54-359b-4b3e-a485-c383ade0c536"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
        "                    epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "31370/31370 [==============================] - 26873s 857ms/step - loss: 1.4636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaElot6YcMn9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "953e866f-2663-47af-8b0c-d650efd1657b"
      },
      "source": [
        "#using the char-rnn model\n",
        "\n",
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) -1)\n",
        "  return tf.one_hot(X, max_id)\n",
        "\n",
        "X_new = preprocess([\"How are yo\"])\n",
        "Y_pred = model.predict_classes(X_new)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-54b7d9aea20a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    X = np.array(tokenizer.texts_to_sequences(texts)) -1)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p3Bk2iacq2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKD6SGKWcwNh"
      },
      "source": [
        "def next_char(text, temperature = 1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :] # pierwszy rząd, ostatni znak, prawdopodobieństwa\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples = 1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, n_chars=50, temperature = 1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text\n",
        "\n",
        "print(complete_text(\"your mother\"), temperature = 0.2)\n",
        "print(complete_text(\"your mother\"), temperature = 1)\n",
        "print(complete_text(\"your mother\"), temperature = 2.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}