{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH16_PART2_with_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAw2jcJF22oW9lHk/Iw8JR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qamtam/Hands-on-machine-learning/blob/main/CH16_PART2_with_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wIKEh-3i262",
        "outputId": "79b02d91-249a-4b3c-f09b-85899ba898dd"
      },
      "source": [
        "#stateless vs stateful RNNS\n",
        "#stateless wyrzuca za przy każdym batchu i przy każdej iteracji i przy każdym epochu stany ukryte w warstwach gru\n",
        "#w wyniku tego za każdym razem zaczyna od zera\n",
        "\n",
        "#staateful mieli pierwszy training batch i wykorzystuje to jako stan ukryty na początku przemielenia drugiego batcha itd.\n",
        "#stateful RNN ma sens tylko i wyłącznie jeśli każdy element batcha zaczyna się tam, gdzie poprzedni się kończy\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "    \n",
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)\n",
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 14.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 8.4MB/s \n",
            "\u001b[?25hDownloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI0j9XbyknF9"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "dataset = dataset.window(window_length, shift = n_steps, drop_remainder = True) # komentarz dziennik II 2020 s 112\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "dataset = dataset.repeat().batch(1)\n",
        "\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) # pierwsze windows to drugi X_batch i Y_batch w drugiej linijce\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsOEMRJwvIVm",
        "outputId": "fd87c90f-5bea-4717-b71f-8433e15e8f45"
      },
      "source": [
        "for (x, y) in dataset.take(1):\n",
        "  print(x,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]], shape=(1, 100, 39), dtype=float32) tf.Tensor(\n",
            "[[ 5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1  0\n",
            "  22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1  4\n",
            "   8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24 17\n",
            "   0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10\n",
            "  15  3 13  0]], shape=(1, 100), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOZoQ0STv1qh"
      },
      "source": [
        "#fancy batches\n",
        "# okno 1 .. 32 (znaki 0 - 32*101) -> batch 1\n",
        "# okno 2 .. 33 -> batch 2\n",
        "\n",
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "for encoded_part in encoded_parts:\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    datasets.append(dataset) # tworzymy listę mikro datasetów\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1I6z3uE0NMJ",
        "outputId": "b4c05a2f-4934-4fa7-ca19-7b72f07b31fb"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                     dropout=0.2, recurrent_dropout=0.2,\n",
        "                     batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXPkFNNJ0Ot_"
      },
      "source": [
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1jWOfZ25XhD",
        "outputId": "6fdaf554-3bdb-49e1-fe00-b72e78d8dfb6"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "steps_per_epoch = train_size // batch_size // n_steps\n",
        "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
        "                    callbacks=[ResetStatesCallback()])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 79s 252ms/step - loss: 2.6145\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 79s 251ms/step - loss: 2.1799\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 78s 248ms/step - loss: 2.5664\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 78s 251ms/step - loss: 2.4927\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 2.2136\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 2.1566\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 2.0477\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 2.0158\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 80s 254ms/step - loss: 1.9707\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 78s 249ms/step - loss: 1.9354\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 78s 249ms/step - loss: 1.9128\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 78s 248ms/step - loss: 1.8915\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 1.8584\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 77s 248ms/step - loss: 1.8380\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 77s 248ms/step - loss: 1.8093\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 77s 245ms/step - loss: 1.7699\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 1.7453\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 77s 247ms/step - loss: 1.7269\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 77s 247ms/step - loss: 1.7104\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 1.6963\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 81s 258ms/step - loss: 1.6851\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 82s 261ms/step - loss: 1.6757\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 80s 257ms/step - loss: 1.6651\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 1.6565\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 1.6501\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 82s 261ms/step - loss: 1.6431\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 1.6353\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 81s 258ms/step - loss: 1.6305\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 81s 260ms/step - loss: 1.6256\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 81s 258ms/step - loss: 1.6211\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 81s 258ms/step - loss: 1.6157\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 81s 260ms/step - loss: 1.6115\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 81s 257ms/step - loss: 1.6071\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 80s 257ms/step - loss: 1.6046\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 80s 254ms/step - loss: 1.6004\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 80s 256ms/step - loss: 1.5978\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 81s 259ms/step - loss: 1.5949\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 80s 254ms/step - loss: 1.5907\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 1.5893\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 1.5862\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 79s 252ms/step - loss: 1.5835\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 1.5801\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 78s 248ms/step - loss: 1.5777\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 1.5761\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 79s 252ms/step - loss: 1.5741\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 79s 253ms/step - loss: 1.5722\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 82s 260ms/step - loss: 1.5697\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 82s 261ms/step - loss: 1.5690\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 80s 255ms/step - loss: 1.5674\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 78s 250ms/step - loss: 1.5653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RuxGAHK5zAU",
        "outputId": "0d0359b6-53e2-480f-fc80-b74c5a4c65a3"
      },
      "source": [
        "# żeby używać modelu startując z fragmentu dowolnej długości należy najpierw stworzyć stateless copy\n",
        "# w przeciwnym razie będzie akceptować wyłącznie fragmenty o długości równej dokładnie długości batcha\n",
        "\n",
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]), #dropout jest zbędny bo i tak jest używany tylko przy treningu\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "stateless_model.build(tf.TensorShape([None, None, max_id])) # model musi zostać zbudowany, żeby ustalić wagi\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "def next_char(text, temperature = 1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :] # pierwszy rząd, ostatni znak, prawdopodobieństwa\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples = 1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, n_chars=50, temperature = 1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text\n",
        "print(complete_text(\"the dog\")) # użycie modelu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your mother:\n",
            "no doing honour this vens with beaught wonst\n",
            "th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm3tPLIU6fK-",
        "outputId": "a9a469bf-bec1-42f4-fa92-2cc1001a4f48"
      },
      "source": [
        "print(complete_text(\"the dog\")) # użycie modelu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the dogging broke;\n",
            "and through conscrieves evension as ty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoNuV1c6ftP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMBKzmBT6k8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-he4OJn6rVy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}