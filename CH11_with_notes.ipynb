{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH11_with_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN85B041g+TLLx1Rzb/fdnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qamtam/Hands-on-machine-learning/blob/main/CH11_with_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUJJ8K57PxcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6c338c3-818c-46cc-cc8b-ca332cc40971"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "\n",
        "\n",
        "# V----- learning rate scheduling ------V \n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
        "#Co 10000 kroków będzie redukcja lr do 0.01/2, 0.01/3 ...\n",
        "\n",
        "#Exponential decay -> co 20 kroków będzie redukcja lr o 10x\n",
        "def exponential_decay(epoch):\n",
        "  return 0.01*0.1**(epoch/20)\n",
        "\n",
        "#funkcja która zwraca elastyczną funkcję\n",
        "def exponential_decay(lr0, s):\n",
        "  def exponential_decay_fn(epoch):\n",
        "    return lr0 * 0.01 ** (epoch /s)\n",
        "  return exponential_decay_fn\n",
        "\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
        "#stworzenie callbacku który będzie dostosywał lr co epoch na podst. naszej funkcji\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "\n",
        "\n",
        "#mikro model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000] /255.0, X_train_full[5000:] /255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_valid, y_valid), callbacks=[lr_scheduler]) #<--- tu się wstawia callback\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.7139 - accuracy: 0.7683 - val_loss: 0.5220 - val_accuracy: 0.8262\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4839 - accuracy: 0.8329 - val_loss: 0.4468 - val_accuracy: 0.8508\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4422 - accuracy: 0.8455 - val_loss: 0.4158 - val_accuracy: 0.8588\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4199 - accuracy: 0.8538 - val_loss: 0.4048 - val_accuracy: 0.8646\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4049 - accuracy: 0.8576 - val_loss: 0.3918 - val_accuracy: 0.8676\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3942 - accuracy: 0.8621 - val_loss: 0.3873 - val_accuracy: 0.8690\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3861 - accuracy: 0.8654 - val_loss: 0.3851 - val_accuracy: 0.8704\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3801 - accuracy: 0.8676 - val_loss: 0.3821 - val_accuracy: 0.8696\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3754 - accuracy: 0.8697 - val_loss: 0.3811 - val_accuracy: 0.8696\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3718 - accuracy: 0.8703 - val_loss: 0.3751 - val_accuracy: 0.8722\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3690 - accuracy: 0.8710 - val_loss: 0.3713 - val_accuracy: 0.8726\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3669 - accuracy: 0.8719 - val_loss: 0.3715 - val_accuracy: 0.8740\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3650 - accuracy: 0.8731 - val_loss: 0.3698 - val_accuracy: 0.8728\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3638 - accuracy: 0.8732 - val_loss: 0.3688 - val_accuracy: 0.8726\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3626 - accuracy: 0.8735 - val_loss: 0.3690 - val_accuracy: 0.8720\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3618 - accuracy: 0.8738 - val_loss: 0.3688 - val_accuracy: 0.8732\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3610 - accuracy: 0.8744 - val_loss: 0.3687 - val_accuracy: 0.8716\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3605 - accuracy: 0.8743 - val_loss: 0.3672 - val_accuracy: 0.8732\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3601 - accuracy: 0.8747 - val_loss: 0.3671 - val_accuracy: 0.8726\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3598 - accuracy: 0.8745 - val_loss: 0.3667 - val_accuracy: 0.8740\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3595 - accuracy: 0.8749 - val_loss: 0.3669 - val_accuracy: 0.8740\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3593 - accuracy: 0.8747 - val_loss: 0.3667 - val_accuracy: 0.8736\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3591 - accuracy: 0.8748 - val_loss: 0.3670 - val_accuracy: 0.8728\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3590 - accuracy: 0.8746 - val_loss: 0.3666 - val_accuracy: 0.8722\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3589 - accuracy: 0.8752 - val_loss: 0.3664 - val_accuracy: 0.8736\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3588 - accuracy: 0.8751 - val_loss: 0.3663 - val_accuracy: 0.8740\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3587 - accuracy: 0.8750 - val_loss: 0.3664 - val_accuracy: 0.8726\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3587 - accuracy: 0.8751 - val_loss: 0.3663 - val_accuracy: 0.8736\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3586 - accuracy: 0.8751 - val_loss: 0.3663 - val_accuracy: 0.8734\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3586 - accuracy: 0.8749 - val_loss: 0.3664 - val_accuracy: 0.8730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPsVsAlKSgCp"
      },
      "source": [
        "# można ustalić learning rate scheduler tak, aby się nadawał do poprzedniej lr (nie tylko lr0), tylko wtedy trzeba gdzie indziej ustalić lr0\n",
        "def exponential_decay_fn(epoch, lr):\n",
        "  return lr*0.1**(1/20)\n",
        "\n",
        "# jeśli chcemy używać performance scheduling (czyli przy plateau w weryfikacji) nalezy używać ReduceLROnPlateau\n",
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "\n",
        "from tensorflow import keras\n",
        "#używamy tf.keras by określić keras.optimizers.schedules zamiast callbacka LearningRateScheduler\n",
        "s = 20 * len(X_train) // 32 # liczba kroków na 32 20 epochów\n",
        "learning_rate= keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
        "optimizer = keras.optimizers.SGD(learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-WqBXqXUuD"
      },
      "source": [
        "# l1/l2\n",
        "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "#wrapper, żeby nie pisać za każdym razem layer = ...\n",
        "\n",
        "from functools import partial\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"elu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "#czyli na przykład dla MNIST...\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Flatten(input_shape=[28,28]),\n",
        "                                 RegularizedDense(300),\n",
        "                                 RegularizedDense(300),\n",
        "                                 RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
        "])\n",
        "\n",
        "#Dropout\n",
        "# ten model by wyrzucał z każdej poprzedniej warstwy 20% neuronów (NIGDY NIE RUSZAĆ OSTATNIEJ WARSTWY)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.Flatten(input_shape=[28,28]),\n",
        "                                 keras.layers.Dropout(rate=0.2),\n",
        "                                 keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "                                 keras.layers.Dropout(rate=0.2),\n",
        "                                 keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "                                 keras.layers.Dropout(rate=0.2), #<-- najważniejszy  dropout dla dużych sieci\n",
        "                                 keras.layers.Dense(10, activation=\"softmax\")\n",
        "                                 ])\n",
        "\n",
        "\n",
        "#MC\n",
        "#jak to działa? robimy 100 przewidzeń na test secie, ale z TRAINING=TRUE!!\n",
        "#Co to znaczy? będziemy mieli 100 różnych sieci (a zatem 100 różnych przedykcji) i je uśredniamy (co daje lepszy rezultat)\n",
        "\n",
        "\n",
        "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)\n",
        "\n",
        "#specjalna wersja dropoutu, by \"wstawić\" ją do test setu jeśli mamy inne warstwy, które mogą zachować się inaczej z training=True\n",
        "class MCDropout(keras.layers.Dropout):\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training=True)\n",
        "\n",
        "  \n",
        "#maxnorm regularization\n",
        "#pilnuje by W było ||w2|| < r (w2 to l2 z w)\n",
        "#przykład poniżej\n",
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La0RGfTEcvrv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}